"""Normalization Layers"""

import torch
import torch.nn as nn

import nn.functional as myF


class GroupNorm(nn.Module):
    def __init__(self, num_groups, num_channels, eps=1e-5, affine=True):
        super().__init__()
        self.num_groups = num_groups
        self.num_channels = num_channels
        self.eps = eps
        self.affine = affine
        if self.affine:
            self.weight = nn.Parameter(torch.Tensor(num_channels))
            self.bias = nn.Parameter(torch.Tensor(num_channels))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        if self.affine:
            self.weight.data.fill_(1)
            self.bias.data.zero_()

    def forward(self, x):
        return myF.group_norm(
            x, self.num_groups, self.weight, self.bias, self.eps
        )

    def extra_repr(self):
        return '{num_groups}, {num_channels}, eps={eps}, ' \
            'affine={affine}'.format(**self.__dict__)


class SwitchNorm(nn.Module):
    __constants__ = ['momentum', 'eps', 'weight', 'bias', 'running_mean', 'running_var']
    def __init__(self, num_features, eps=1e-5, momentum=0.997, using_moving_average=True,
                 using_bn=True, last_gamma=False):
        super(SwitchNorm, self).__init__()
        self.weight = nn.Parameter(torch.ones(1,num_features,1,1))
        self.bias = nn.Parameter(torch.zeros(1,num_features,1,1))
        if using_bn:
            self.mean_weight = nn.Parameter(torch.ones(3))
            self.var_weight = nn.Parameter(torch.ones(3))
        else:
            self.mean_weight = nn.Parameter(torch.ones(2))
            self.var_weight = nn.Parameter(torch.ones(2))
        self.eps = eps
        self.momentum = momentum
        self.using_moving_average = using_moving_average
        self.using_bn = using_bn
        self.last_gamma = last_gamma
        if self.using_bn:
            self.register_buffer('running_mean', torch.zeros(1, num_features, 1))
            self.register_buffer('running_var', torch.zeros(1, num_features, 1))
        if self.using_bn and (not self.using_moving_average):
            self.register_buffer('batch_mean', torch.zeros(1, num_features, 1))
            self.register_buffer('batch_var', torch.zeros(1, num_features, 1))
        self.reset_parameters()

    def reset_parameters(self):
        if self.using_bn:
            self.running_mean.zero_()
            self.running_var.zero_()
        if self.using_bn and (not self.using_moving_average):
            self.batch_mean.zero_()
            self.batch_var.zero_()
        if self.last_gamma:
            self.weight.data.fill_(0)
        else:
            self.weight.data.fill_(1)
        self.bias.data.zero_()

    def forward(self, x):
        N, C, H, W = x.size()
        x = x.view(N, C, -1)
        mean_in = x.mean(-1, keepdim=True)
        var_in = x.var(-1, keepdim=True)

        mean_ln = mean_in.mean(1, keepdim=True)
        temp = var_in + mean_in ** 2
        var_ln = temp.mean(1, keepdim=True) - mean_ln ** 2

        if self.using_bn:
            if self.training:
                mean_bn = mean_in.mean(0, keepdim=True)
                var_bn = temp.mean(0, keepdim=True) - mean_bn ** 2
                if self.using_moving_average:
                    self.running_mean.mul_(self.momentum)
                    self.running_mean.add_((1 - self.momentum) * mean_bn.data)
                    self.running_var.mul_(self.momentum)
                    self.running_var.add_((1 - self.momentum) * var_bn.data)
                else:
                    self.batch_mean.add_(mean_bn.data)
                    self.batch_var.add_(mean_bn.data ** 2 + var_bn.data)
            else:
                mean_bn = torch.autograd.Variable(self.running_mean)
                var_bn = torch.autograd.Variable(self.running_var)

        softmax = nn.Softmax(0)
        mean_weight = softmax(self.mean_weight)
        var_weight = softmax(self.var_weight)

        if self.using_bn:
            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn
            var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn
        else:
            mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln
            var = var_weight[0] * var_in + var_weight[1] * var_ln

        x = (x-mean) / (var+self.eps).sqrt()
        x = x.view(N, C, H, W)
        return x * self.weight + self.bias

    # def forward(self, x):
    #     N, C, H, W = x.size()
    #     tmp_mean = x.mean(-1)
    #     tmp_var = x.var(-1)

    #     mean_in = tmp_mean.mean(-1, keepdim=True)
    #     var_in = (tmp_var + tmp_mean ** 2).mean(-1, keepdim=True) - mean_in ** 2

    #     mean_ln = mean_in.mean(1, keepdim=True)
    #     temp = var_in + mean_in ** 2
    #     var_ln = temp.mean(1, keepdim=True) - mean_ln ** 2

    #     if self.using_bn:
    #         if self.training:
    #             mean_bn = mean_in.mean(0, keepdim=True)
    #             var_bn = temp.mean(0, keepdim=True) - mean_bn ** 2
    #             if self.using_moving_average:
    #                 self.running_mean.mul_(self.momentum)
    #                 self.running_mean.add_((1 - self.momentum) * mean_bn.data)
    #                 self.running_var.mul_(self.momentum)
    #                 self.running_var.add_((1 - self.momentum) * var_bn.data)
    #             else:
    #                 self.batch_mean.add_(mean_bn.data)
    #                 self.batch_var.add_(mean_bn.data ** 2 + var_bn.data)
    #         else:
    #             mean_bn = torch.autograd.Variable(self.running_mean)
    #             var_bn = torch.autograd.Variable(self.running_var)

    #     softmax = nn.Softmax(0)
    #     mean_weight = softmax(self.mean_weight)
    #     var_weight = softmax(self.var_weight)

    #     if self.using_bn:
    #         mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln + mean_weight[2] * mean_bn
    #         var = var_weight[0] * var_in + var_weight[1] * var_ln + var_weight[2] * var_bn
    #     else:
    #         mean = mean_weight[0] * mean_in + mean_weight[1] * mean_ln
    #         var = var_weight[0] * var_in + var_weight[1] * var_ln

    #     x = (x-mean.unsqueeze(-1)) / (var.unsqueeze(-1)+self.eps).sqrt()
    #     # x = x.view(N, C, H, W)
    #     return x.mul_(self.weight).add_(self.bias)
    #     # return (x.sub_(mean.unsqueeze(-1).div_(var.unsqueeze(-1)+self.eps).sqrt_())).mul_(self.weight).add_(self.bias)

